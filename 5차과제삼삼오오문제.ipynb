{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 개념 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-1. SentiSynset 객체는 ( 긍정감성 )지수와 ( 부정감성 )지수를 가지고 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-2. ( )는 소셜 미디어의 감성 분석 용도로 만들어진 룰 기반의 Lexicon이다. 이를 이용하면 먼저 SentimentIntensityAnalyzer 객체를 생성한 뒤에 문서별로 ( )메서드를 호출해 감성 점수를 구한 뒤, 해당 문서의 감성 점수가 특정 임계값 이상이면( ), 그렇지 않으면 ( )으로 판단한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-3. 텍스트 분류 기반의 문서 분류와 달리 문서 군집화는 학습 데이터 세트가 필요없는 (     ) 으로 동작한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-4. 사이킷런의 KMenas 객체는 각 군집을 구성하는 단어 피처가 군집의 중심 ( ) 을 기준으로 얼마나 가깝게 위치해 있는지 ( ) 라는 속성으로 제공한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-5. (  ) 란 최상위 루트로부터 경유한 경로를 전부 기입하는 방식이며 어떤 파일이 가진 고유한 경로를 뜻한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 감성분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-1. 주어진 텍스트를 아래와 같이 변환하기 위해 빈칸을 채워주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "review_df = pd.read_csv('./labeledTrainData.tsv', header=0, sep=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n"
     ]
    }
   ],
   "source": [
    "print(review_df['review'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "review_df['review'] = review_df['review'].str.replace('<br />',' ')\n",
    "\n",
    "review_df['review'] = review_df['review'].apply( lambda x : re.sub(\"[^a-zA-Z]\", \" \", x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " With all this stuff going down at the moment with MJ i ve started listening to his music  watching the odd documentary here and there  watched The Wiz and watched Moonwalker again  Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent  Moonwalker is part biography  part feature film which i remember going to see at the cinema when it was originally released  Some of it has subtle messages about MJ s feeling towards the press and also the obvious message of drugs are bad m kay   Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring  Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him   The actual feature film bit when it finally starts is only on for    minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord  Why he wants MJ dead so bad is beyond me  Because MJ overheard his plans  Nah  Joe Pesci s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno  maybe he just hates MJ s music   Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence  Also  the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene   Bottom line  this movie is for people who like MJ on one level or another  which i think is most people   If not  then stay away  It does try and give off a wholesome message and ironically MJ s bestest buddy in this movie is a girl  Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty  Well  with all the attention i ve gave this subject    hmmm well i don t know because people can be different behind closed doors  i know this for a fact  He is either an extremely nice but stupid guy or one of the most sickest liars  I hope he is not the latter  \n"
     ]
    }
   ],
   "source": [
    "print(review_df['review'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-2. 'alarm'의 긍정감성지수, 부정감성지수, 객관성지수를 확인해봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\jaeyong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\sentiwordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alarm 긍정감성 지수:  0.375\n",
      "alarm 부정감성 지수:  0.25\n",
      "alarm 객관성 지수:  0.375\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "alarm = swn.senti_synset('alarm.n.01')\n",
    "\n",
    "print('alarm 긍정감성 지수: ', alarm.pos_score())\n",
    "print('alarm 부정감성 지수: ', alarm.neg_score())\n",
    "print('alarm 객관성 지수: ', alarm.obj_score())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alarm 긍정감성 지수: 0.375\n",
    "\n",
    "alarm 부정감성 지수: 0.25\n",
    "\n",
    "alarm 객관성 지수: 0.375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-3. 빈칸을 채우며 SentiWordNet을 이용한 감성분석과 VADER를 이용한 감성분석 과정을 복습해봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jaeyong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>The Classic War of the Worlds   by Timothy ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager  Nicholas Bell...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised thi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"8196_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>I dont know why people think this is such a b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"7166_2\"</td>\n",
       "      <td>0</td>\n",
       "      <td>This movie could have been very good  but com...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"10633_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>I watched this video at a friend s house  I m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"319_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>A friend of mine bought this film for     and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"8713_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>This movie is full of references  Like   Ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sentiment                                             review  \\\n",
       "0   \"5814_8\"          1   With all this stuff going down at the moment ...   \n",
       "1   \"2381_9\"          1     The Classic War of the Worlds   by Timothy ...   \n",
       "2   \"7759_3\"          0   The film starts with a manager  Nicholas Bell...   \n",
       "3   \"3630_4\"          0   It must be assumed that those who praised thi...   \n",
       "4   \"9495_8\"          1   Superbly trashy and wondrously unpretentious ...   \n",
       "5   \"8196_8\"          1   I dont know why people think this is such a b...   \n",
       "6   \"7166_2\"          0   This movie could have been very good  but com...   \n",
       "7  \"10633_1\"          0   I watched this video at a friend s house  I m...   \n",
       "8    \"319_1\"          0   A friend of mine bought this film for     and...   \n",
       "9  \"8713_10\"          1     This movie is full of references  Like   Ma...   \n",
       "\n",
       "   preds  \n",
       "0      0  \n",
       "1      1  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "5      1  \n",
       "6      0  \n",
       "7      0  \n",
       "8      0  \n",
       "9      1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return\n",
    "\n",
    "def swn_polarity(text):\n",
    "    sentiment = 0.0\n",
    "    tokens_count = 0\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    raw_sentences = sent_tokenize(text)\n",
    "    \n",
    "    # 분해된 문장별로 단어 토큰 -> 품사 태깅 후에 SentiSynset 생성 -> 감성 지수 합산 \n",
    "    for raw_sentence in raw_sentences:\n",
    "        # NTLK 기반의 품사 태깅 문장 추출  \n",
    "        tagged_sentence = pos_tag(word_tokenize(raw_sentence))\n",
    "        for word , tag in tagged_sentence:\n",
    "            \n",
    "            # WordNet 기반 품사 태깅과 어근 추출\n",
    "            wn_tag = penn_to_wn(tag)\n",
    "            if wn_tag not in (wn.NOUN , wn.ADJ, wn.ADV):\n",
    "                continue                   \n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            if not lemma:\n",
    "                continue\n",
    "            \n",
    "            # 어근을 추출한 단어와 WordNet 기반 품사 태깅을 입력해 Synset 객체를 생성\n",
    "            synsets = wn.synsets(lemma , pos=wn_tag)\n",
    "            if not synsets:\n",
    "                continue\n",
    "            \n",
    "            # sentiwordnet의 감성 단어 분석으로 감성 synset 추출\n",
    "            synset = synsets[0]\n",
    "            swn_synset = swn.senti_synset(synset.name())\n",
    "            sentiment += (swn_synset.pos_score() - swn_synset.neg_score())           \n",
    "            tokens_count += 1          \n",
    "    \n",
    "    if not tokens_count:\n",
    "        return 0\n",
    "    \n",
    "    if sentiment >= 0 :\n",
    "        return 1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "review_df['preds'] = review_df['review'].apply( lambda x : swn_polarity(x) )\n",
    "review_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\jaeyong\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>preds</th>\n",
       "      <th>vader_preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>The Classic War of the Worlds   by Timothy ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager  Nicholas Bell...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised thi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"8196_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>I dont know why people think this is such a b...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"7166_2\"</td>\n",
       "      <td>0</td>\n",
       "      <td>This movie could have been very good  but com...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"10633_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>I watched this video at a friend s house  I m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"319_1\"</td>\n",
       "      <td>0</td>\n",
       "      <td>A friend of mine bought this film for     and...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"8713_10\"</td>\n",
       "      <td>1</td>\n",
       "      <td>This movie is full of references  Like   Ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sentiment                                             review  \\\n",
       "0   \"5814_8\"          1   With all this stuff going down at the moment ...   \n",
       "1   \"2381_9\"          1     The Classic War of the Worlds   by Timothy ...   \n",
       "2   \"7759_3\"          0   The film starts with a manager  Nicholas Bell...   \n",
       "3   \"3630_4\"          0   It must be assumed that those who praised thi...   \n",
       "4   \"9495_8\"          1   Superbly trashy and wondrously unpretentious ...   \n",
       "5   \"8196_8\"          1   I dont know why people think this is such a b...   \n",
       "6   \"7166_2\"          0   This movie could have been very good  but com...   \n",
       "7  \"10633_1\"          0   I watched this video at a friend s house  I m...   \n",
       "8    \"319_1\"          0   A friend of mine bought this film for     and...   \n",
       "9  \"8713_10\"          1     This movie is full of references  Like   Ma...   \n",
       "\n",
       "   preds  vader_preds  \n",
       "0      0            0  \n",
       "1      1            1  \n",
       "2      0            0  \n",
       "3      0            1  \n",
       "4      0            1  \n",
       "5      1            1  \n",
       "6      0            0  \n",
       "7      0            0  \n",
       "8      0            1  \n",
       "9      1            1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def vader_polarity(review,threshold=0.1):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = analyzer.polarity_scores(review)\n",
    "    \n",
    "    # compound 값에 기반하여 threshold 입력값보다 크면 1, 그렇지 않으면 0을 반환 \n",
    "    agg_score = scores['compound']\n",
    "    final_sentiment = 1 if agg_score >= threshold else 0\n",
    "    return final_sentiment\n",
    "\n",
    "review_df['vader_preds'] = review_df['review'].apply( lambda x : vader_polarity(x, 0.1) )\n",
    "review_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score \n",
    "from sklearn.metrics import recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def get_clf_eval(y_test=None, pred=None):\n",
    "    confusion = confusion_matrix( y_test, pred)\n",
    "    accuracy = accuracy_score(y_test , pred)\n",
    "    precision = precision_score(y_test , pred)\n",
    "    recall = recall_score(y_test , pred)\n",
    "    f1 = f1_score(y_test,pred)\n",
    "    roc_auc = roc_auc_score(y_test, pred)\n",
    "    print('오차 행렬')\n",
    "    print(confusion)\n",
    "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n",
    "    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### SentiWordNet 예측 성능 평가 ####\n",
      "오차 행렬\n",
      "[[7668 4832]\n",
      " [3636 8864]]\n",
      "정확도: 0.6613, 정밀도: 0.6472, 재현율: 0.7091,    F1: 0.6767, AUC:0.6613\n",
      "#### VADER 예측 성능 평가 ####\n",
      "오차 행렬\n",
      "[[ 6736  5764]\n",
      " [ 1867 10633]]\n",
      "정확도: 0.6948, 정밀도: 0.6485, 재현율: 0.8506,    F1: 0.7359, AUC:0.6948\n"
     ]
    }
   ],
   "source": [
    "y1_target = review_df['sentiment'].values\n",
    "preds = review_df['preds'].values\n",
    "print('#### SentiWordNet 예측 성능 평가 ####')\n",
    "get_clf_eval(y1_target, preds)\n",
    "\n",
    "y2_target = review_df['sentiment'].values\n",
    "vader_preds = review_df['vader_preds'].values\n",
    "print('#### VADER 예측 성능 평가 ####')\n",
    "get_clf_eval(y2_target, vader_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 간단한 토픽 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 짧은 sentence를 가지고 LDA를 이용해 간단한 토픽 모델링을 진행해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'In the 200th episode of The Life Scientific, Jim Al-Khalili finds out why Demis Hassabis wants to create artificial intelligence and use it to help humanity. Thinking about how to win at chess when he was a boy got Demis thinking about the process of thinking itself. Being able to program his first computer (a Sinclair Spectrum) felt miraculous. In computer chess, his two passions were combined. And a lifelong ambition to create artificial intelligence was born. Demis studied computer science at Cambridge and then worked in the computer games industry for many years. Games, he says, are the ideal testing ground for AI. Then, thinking memory and imagination were aspects of the human mind that would be a necessary part of any artificially intelligent system, he studied neuroscience for a PhD. He set up DeepMind in 2010 and pioneered a new approach to creating artificial intelligence, based on deep learning and built-in rewards for making good decisions. Four years later, DeepMind was sold to Google for £400 million. The company’s landmark creation, Alpha Go stunned the world when it defeated the world Go champion in South Korea in 2016. Their AI system, AlphaZero taught itself to play chess from scratch. After playing against itself for just four hours, it was the best chess computer in the world. (Humans had been defeated long ago). Many fear both the supreme intelligence and the stupidity of AI. Demis imagines a future in which computers and humans put their brains together to try and understand the world. His algorithms have inspired humans to raise their game, when playing Go and chess. Now, he hopes that AI might do the same for scientific research. Perhaps the next Nobel Prize will be shared between a human and AI? Producer: Anna Buckley'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. #___________ 를 채워 넣어 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['In', 'the', '200th', 'episode', 'of', 'The', 'Life', 'Scientific', ',', 'Jim', 'Al-Khalili', 'finds', 'out', 'why', 'Demis', 'Hassabis', 'wants', 'to', 'create', 'artificial', 'intelligence', 'and', 'use', 'it', 'to', 'help', 'humanity', '.'], ['Thinking', 'about', 'how', 'to', 'win', 'at', 'chess', 'when', 'he', 'was', 'a', 'boy', 'got', 'Demis', 'thinking', 'about', 'the', 'process', 'of', 'thinking', 'itself', '.'], ['Being', 'able', 'to', 'program', 'his', 'first', 'computer', '(', 'a', 'Sinclair', 'Spectrum', ')', 'felt', 'miraculous', '.'], ['In', 'computer', 'chess', ',', 'his', 'two', 'passions', 'were', 'combined', '.'], ['And', 'a', 'lifelong', 'ambition', 'to', 'create', 'artificial', 'intelligence', 'was', 'born', '.'], ['Demis', 'studied', 'computer', 'science', 'at', 'Cambridge', 'and', 'then', 'worked', 'in', 'the', 'computer', 'games', 'industry', 'for', 'many', 'years', '.'], ['Games', ',', 'he', 'says', ',', 'are', 'the', 'ideal', 'testing', 'ground', 'for', 'AI', '.'], ['Then', ',', 'thinking', 'memory', 'and', 'imagination', 'were', 'aspects', 'of', 'the', 'human', 'mind', 'that', 'would', 'be', 'a', 'necessary', 'part', 'of', 'any', 'artificially', 'intelligent', 'system', ',', 'he', 'studied', 'neuroscience', 'for', 'a', 'PhD', '.'], ['He', 'set', 'up', 'DeepMind', 'in', '2010', 'and', 'pioneered', 'a', 'new', 'approach', 'to', 'creating', 'artificial', 'intelligence', ',', 'based', 'on', 'deep', 'learning', 'and', 'built-in', 'rewards', 'for', 'making', 'good', 'decisions', '.'], ['Four', 'years', 'later', ',', 'DeepMind', 'was', 'sold', 'to', 'Google', 'for', '£400', 'million', '.'], ['The', 'company', '’', 's', 'landmark', 'creation', ',', 'Alpha', 'Go', 'stunned', 'the', 'world', 'when', 'it', 'defeated', 'the', 'world', 'Go', 'champion', 'in', 'South', 'Korea', 'in', '2016', '.'], ['Their', 'AI', 'system', ',', 'AlphaZero', 'taught', 'itself', 'to', 'play', 'chess', 'from', 'scratch', '.'], ['After', 'playing', 'against', 'itself', 'for', 'just', 'four', 'hours', ',', 'it', 'was', 'the', 'best', 'chess', 'computer', 'in', 'the', 'world', '.'], ['(', 'Humans', 'had', 'been', 'defeated', 'long', 'ago', ')', '.'], ['Many', 'fear', 'both', 'the', 'supreme', 'intelligence', 'and', 'the', 'stupidity', 'of', 'AI', '.'], ['Demis', 'imagines', 'a', 'future', 'in', 'which', 'computers', 'and', 'humans', 'put', 'their', 'brains', 'together', 'to', 'try', 'and', 'understand', 'the', 'world', '.'], ['His', 'algorithms', 'have', 'inspired', 'humans', 'to', 'raise', 'their', 'game', ',', 'when', 'playing', 'Go', 'and', 'chess', '.'], ['Now', ',', 'he', 'hopes', 'that', 'AI', 'might', 'do', 'the', 'same', 'for', 'scientific', 'research', '.'], ['Perhaps', 'the', 'next', 'Nobel', 'Prize', 'will', 'be', 'shared', 'between', 'a', 'human', 'and', 'AI', '?'], ['Producer', ':', 'Anna', 'Buckley']]\n"
     ]
    }
   ],
   "source": [
    "word_tokens = tokenizer(sentence)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. print 값을 참고해서 stop word를 제거하고 1차원 filtered_words 리스트를 완성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "filtered_words  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in word_tokens:\n",
    "    for word in sentence:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['200th', 'episode', 'life', 'scientific', ',', 'jim', 'al-khalili', 'finds', 'demis', 'hassabis', 'wants', 'create', 'artificial', 'intelligence', 'use', 'help', 'humanity', '.', 'thinking', 'win', 'chess', 'boy', 'got', 'demis', 'thinking', 'process', 'thinking', '.', 'able', 'program', 'first', 'computer', '(', 'sinclair', 'spectrum', ')', 'felt', 'miraculous', '.', 'computer', 'chess', ',', 'two', 'passions', 'combined', '.', 'lifelong', 'ambition', 'create', 'artificial', 'intelligence', 'born', '.', 'demis', 'studied', 'computer', 'science', 'cambridge', 'worked', 'computer', 'games', 'industry', 'many', 'years', '.', 'games', ',', 'says', ',', 'ideal', 'testing', 'ground', 'ai', '.', ',', 'thinking', 'memory', 'imagination', 'aspects', 'human', 'mind', 'would', 'necessary', 'part', 'artificially', 'intelligent', 'system', ',', 'studied', 'neuroscience', 'phd', '.', 'set', 'deepmind', '2010', 'pioneered', 'new', 'approach', 'creating', 'artificial', 'intelligence', ',', 'based', 'deep', 'learning', 'built-in', 'rewards', 'making', 'good', 'decisions', '.', 'four', 'years', 'later', ',', 'deepmind', 'sold', 'google', '£400', 'million', '.', 'company', '’', 'landmark', 'creation', ',', 'alpha', 'go', 'stunned', 'world', 'defeated', 'world', 'go', 'champion', 'south', 'korea', '2016', '.', 'ai', 'system', ',', 'alphazero', 'taught', 'play', 'chess', 'scratch', '.', 'playing', 'four', 'hours', ',', 'best', 'chess', 'computer', 'world', '.', '(', 'humans', 'defeated', 'long', 'ago', ')', '.', 'many', 'fear', 'supreme', 'intelligence', 'stupidity', 'ai', '.', 'demis', 'imagines', 'future', 'computers', 'humans', 'put', 'brains', 'together', 'try', 'understand', 'world', '.', 'algorithms', 'inspired', 'humans', 'raise', 'game', ',', 'playing', 'go', 'chess', '.', ',', 'hopes', 'ai', 'might', 'scientific', 'research', '.', 'perhaps', 'next', 'nobel', 'prize', 'shared', 'human', 'ai', '?', 'producer', ':', 'anna', 'buckley']\n"
     ]
    }
   ],
   "source": [
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=1, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=10, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "feat_vect = count_vect.fit_transform(filtered_words)\n",
    "lda = LatentDirichletAllocation(n_components = 1, random_state = 10)\n",
    "lda.fit(feat_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. stop word를 제거한 후 남은 word 피처의 개수는 몇 개인가요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 : 211"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4. display_topics 함수를 완성하여 연관도가 높은 순으로 Word를 나열하는 함수를 만들어주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, features_names, no_top_words):\n",
    "     for topic_index, topic in enumerate(model.components_):\n",
    "        print('Topic #',topic_index)\n",
    "\n",
    "        topic_word_indexes = topic.argsort()[::-1]\n",
    "        top_indexes=topic_word_indexes[:no_top_words]\n",
    "        \n",
    "        feature_concat = ' '.join([feature_names[i] for i in top_indexes])                \n",
    "        print(feature_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic # 0\n",
      "ai chess computer intelligence world demis thinking artificial humans go\n"
     ]
    }
   ],
   "source": [
    "feature_names = count_vect.get_feature_names()\n",
    "display_topics(lda, feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-5. 출력 결과 단어 10개를 적어주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10개 word : ai chess computer intelligence world demis thinking artificial humans go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
